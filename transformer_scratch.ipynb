{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "127c0847",
   "metadata": {},
   "source": [
    "## Transformer model (not pre-trained)\n",
    "\n",
    "### Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52a23bdb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22585,
     "status": "ok",
     "timestamp": 1758583914401,
     "user": {
      "displayName": "John-Allan Ellingson",
      "userId": "17955604035337839024"
     },
     "user_tz": 240
    },
    "id": "52a23bdb",
    "outputId": "39e155d5-134c-40bb-eb9c-f64db167067a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from google.colab import drive, runtime\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5db8554",
   "metadata": {},
   "source": [
    "### Data structure definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391adfb0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1257,
     "status": "ok",
     "timestamp": 1758583918210,
     "user": {
      "displayName": "John-Allan Ellingson",
      "userId": "17955604035337839024"
     },
     "user_tz": 240
    },
    "id": "391adfb0",
    "outputId": "e2150c85-4f9f-4692-daff-7b574fc022d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data file found at /content/drive/MyDrive/AASD4011 Models/JA DoyleX/doyle_preprocessed_data.pkl.\n",
      "Write test succeeded for: /content/drive/MyDrive/AASD4011 Models/JA DoyleX/doyle_preprocessed_data_test.pkl\n",
      "Write test succeeded for: /content/drive/MyDrive/AASD4011 Models/JA DoyleX/doyleX_model_test.keras\n",
      "Write test succeeded for: /content/drive/MyDrive/AASD4011 Models/JA DoyleX/doyleX_batch_losses_test.npy\n",
      "Write test succeeded for: /content/drive/MyDrive/AASD4011 Models/JA DoyleX/doyleX_training_history_test.pkl\n"
     ]
    }
   ],
   "source": [
    "base_corpus = \"doyle\"\n",
    "\n",
    "#directory from Google Colab, where model was trained\n",
    "dir = f\"/content/drive/MyDrive/AASD4011 Models/JA DoyleX\"\n",
    "\n",
    "#Output file paths and extensions\n",
    "filepaths = {\n",
    "    \"data\": f\"{dir}/{base_corpus}_preprocessed_data\",\n",
    "    \"model\": f\"{dir}/doyleX_model\",\n",
    "    \"batch\": f\"{dir}/doyleX_batch_losses\",\n",
    "    \"history\": f\"{dir}/doyleX_training_history\",\n",
    "    \"checkpoint\": f\"{dir}/doyleX_weights\"\n",
    "}\n",
    "extensions = {\n",
    "    \"data\": \".pkl\",\n",
    "    \"model\": \".keras\",\n",
    "    \"batch\": \".npy\",\n",
    "    \"history\": \".pkl\",\n",
    "    \"checkpoint\": \".weights.h5\"\n",
    "}\n",
    "data_filepath = filepaths[\"data\"] + extensions[\"data\"]\n",
    "model_filepath = filepaths[\"model\"] + extensions[\"model\"]\n",
    "batch_filepath = filepaths[\"batch\"] + extensions[\"batch\"]\n",
    "history_filepath = filepaths[\"history\"] + extensions[\"history\"]\n",
    "checkpoint_filepath = filepaths[\"checkpoint\"] + extensions[\"checkpoint\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53db8182",
   "metadata": {},
   "source": [
    "### Colab Tests and validation\n",
    "\n",
    "Check file I/O before beginning training, to avoid wasting resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b177d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for data file existence\n",
    "import os\n",
    "if not os.path.exists(data_filepath):\n",
    "    raise FileNotFoundError(f\"Data file not found at {data_filepath}.\")\n",
    "else:\n",
    "    print(f\"Data file found at {data_filepath}.\")\n",
    "\n",
    "#Check that a file can be saved to model_filepath and history_filepath\n",
    "for path in [filepaths[\"data\"] + \"_test\" + extensions[\"data\"],\n",
    "             filepaths[\"model\"] + \"_test\" + extensions[\"model\"],\n",
    "             filepaths[\"batch\"] + \"_test\" + extensions[\"batch\"],\n",
    "             filepaths[\"history\"] + \"_test\" + extensions[\"history\"]]:\n",
    "    try:\n",
    "        with open(path, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(path)\n",
    "        print(f\"Write test succeeded for: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Write test FAILED for: {path} -- {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6486d32c",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a40be8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2716,
     "status": "ok",
     "timestamp": 1758583924894,
     "user": {
      "displayName": "John-Allan Ellingson",
      "userId": "17955604035337839024"
     },
     "user_tz": 240
    },
    "id": "27a40be8",
    "outputId": "fbd9d150-75d7-4df0-8ed1-93aee2b6d240"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 40462\n",
      "Max sequence length: 40\n"
     ]
    }
   ],
   "source": [
    "#Load preprocessed data from subdirectory\n",
    "with open(data_filepath, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    X_train = data[\"X_train\"]\n",
    "    y_train = data[\"y_train\"]\n",
    "    word_to_id = data[\"word_to_id\"]\n",
    "    id_to_word = data[\"id_to_word\"]\n",
    "    max_seq_length = data[\"max_seq_length\"]\n",
    "    attention_masks = data[\"attention_masks\"] if \"attention_masks\" in data else None\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "attention_masks = np.array(attention_masks)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Max sequence length: {max_seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d197a158",
   "metadata": {},
   "source": [
    "### Transformer (non-GPT) definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67615550",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2996582,
     "status": "ok",
     "timestamp": 1758590158886,
     "user": {
      "displayName": "John-Allan Ellingson",
      "userId": "17955604035337839024"
     },
     "user_tz": 240
    },
    "id": "67615550",
    "outputId": "6e527cf0-cdb3-441c-a50d-9852a9f20c5d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'positional_encoding_1' (of type PositionalEncoding) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,179,136</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_4       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ positional_encodin… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncodin…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>,   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">263,808</span> │ positional_encod… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ positional_encod… │\n",
       "│                     │                   │            │ lambda_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ positional_encod… │\n",
       "│                     │                   │            │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,920</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequential_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40462</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,219,598</span> │ layer_normalizat… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m5,179,136\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_4       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ positional_encodin… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mPositionalEncodin…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_1 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m,   │          \u001b[38;5;34m0\u001b[0m │ input_layer_4[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m40\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m263,808\u001b[0m │ positional_encod… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ positional_encod… │\n",
       "│                     │                   │            │ lambda_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_2 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ positional_encod… │\n",
       "│                     │                   │            │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │        \u001b[38;5;34m256\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │     \u001b[38;5;34m65,920\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ sequential_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_3 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │        \u001b[38;5;34m256\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40462\u001b[0m) │  \u001b[38;5;34m5,219,598\u001b[0m │ layer_normalizat… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,728,974</span> (40.93 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,728,974\u001b[0m (40.93 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,728,974</span> (40.93 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,728,974\u001b[0m (40.93 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1331/1331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 113ms/step - accuracy: 0.2013 - loss: 1.9378 - val_accuracy: 0.4444 - val_loss: 0.3078\n",
      "Epoch 2/20\n",
      "\u001b[1m1331/1331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 112ms/step - accuracy: 0.4348 - loss: 0.1989 - val_accuracy: 0.4640 - val_loss: 0.1541\n",
      "Epoch 3/20\n",
      "\u001b[1m1331/1331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 113ms/step - accuracy: 0.4481 - loss: 0.0817 - val_accuracy: 0.4684 - val_loss: 0.1231\n",
      "Epoch 4/20\n",
      "\u001b[1m1331/1331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 112ms/step - accuracy: 0.4536 - loss: 0.0432 - val_accuracy: 0.4713 - val_loss: 0.1070\n",
      "Epoch 5/20\n",
      "\u001b[1m1331/1331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 112ms/step - accuracy: 0.4559 - loss: 0.0228 - val_accuracy: 0.4724 - val_loss: 0.1018\n",
      "Epoch 6/20\n",
      "\u001b[1m1331/1331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 112ms/step - accuracy: 0.4587 - loss: 0.0131 - val_accuracy: 0.4728 - val_loss: 0.1070\n",
      "Epoch 7/20\n",
      "\u001b[1m1331/1331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 112ms/step - accuracy: 0.4597 - loss: 0.0080 - val_accuracy: 0.4734 - val_loss: 0.1093\n",
      "Epoch 8/20\n",
      "\u001b[1m1331/1331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 112ms/step - accuracy: 0.4592 - loss: 0.0049 - val_accuracy: 0.4738 - val_loss: 0.1159\n",
      "Epoch 9/20\n",
      "\u001b[1m1331/1331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 112ms/step - accuracy: 0.4598 - loss: 0.0036 - val_accuracy: 0.4740 - val_loss: 0.1193\n",
      "Epoch 10/20\n",
      "\u001b[1m1331/1331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 112ms/step - accuracy: 0.4604 - loss: 0.0028 - val_accuracy: 0.4745 - val_loss: 0.1227\n",
      "Epoch 11/20\n",
      "\u001b[1m1331/1331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 113ms/step - accuracy: 0.4602 - loss: 0.0023 - val_accuracy: 0.4746 - val_loss: 0.1270\n",
      "Epoch 12/20\n",
      "\u001b[1m1331/1331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 112ms/step - accuracy: 0.4593 - loss: 0.0021 - val_accuracy: 0.4749 - val_loss: 0.1258\n",
      "Epoch 13/20\n",
      "\u001b[1m1331/1331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 112ms/step - accuracy: 0.4602 - loss: 0.0020 - val_accuracy: 0.4744 - val_loss: 0.1401\n",
      "Epoch 14/20\n",
      "\u001b[1m1331/1331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 111ms/step - accuracy: 0.4608 - loss: 0.0018 - val_accuracy: 0.4750 - val_loss: 0.1282\n",
      "Epoch 15/20\n",
      "\u001b[1m1331/1331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 112ms/step - accuracy: 0.4596 - loss: 0.0015 - val_accuracy: 0.4752 - val_loss: 0.1343\n",
      "Epoch 16/20\n",
      "\u001b[1m1331/1331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 112ms/step - accuracy: 0.4601 - loss: 0.0014 - val_accuracy: 0.4755 - val_loss: 0.1293\n",
      "Epoch 17/20\n",
      "\u001b[1m1331/1331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 112ms/step - accuracy: 0.4608 - loss: 0.0014 - val_accuracy: 0.4756 - val_loss: 0.1312\n",
      "Epoch 18/20\n",
      "\u001b[1m1331/1331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 112ms/step - accuracy: 0.4605 - loss: 0.0011 - val_accuracy: 0.4758 - val_loss: 0.1340\n",
      "Epoch 19/20\n",
      "\u001b[1m1331/1331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 111ms/step - accuracy: 0.4600 - loss: 9.9157e-04 - val_accuracy: 0.4758 - val_loss: 0.1300\n",
      "Epoch 20/20\n",
      "\u001b[1m1331/1331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 112ms/step - accuracy: 0.4618 - loss: 9.9271e-04 - val_accuracy: 0.4759 - val_loss: 0.1287\n"
     ]
    }
   ],
   "source": [
    "# Positional Encoding Layer (as defined in the original notebook)\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    \"\"\"\n",
    "    Adds positional encoding to the input embeddings.\n",
    "    Uses sine for even indices and cosine for odd indices of the embedding dimension.\n",
    "    Reference: \"Attention is All You Need\" (Vaswani et al., 2017).\n",
    "    \"\"\"\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "        pos = np.arange(max_len)[:, np.newaxis]\n",
    "        i = np.arange(d_model)[np.newaxis, :]\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        angle_rads = pos * angle_rates\n",
    "        pos_encoding = np.zeros((max_len, d_model))\n",
    "        pos_encoding[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        pos_encoding[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        self.pos_encoding = tf.constant(pos_encoding[np.newaxis, ...], dtype=tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        return x + self.pos_encoding[:, :tf.shape(x)[1], :]\n",
    "\n",
    "# Model Hyperparameters\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "ff_dim = 256\n",
    "dropout_rate = 0.1\n",
    "#vocab size defined above\n",
    "#max_seq_length defined above\n",
    "\n",
    "# Model Definition\n",
    "inputs = keras.Input(shape=(max_seq_length,), dtype=\"int32\")\n",
    "mask_inputs = keras.Input(shape=(max_seq_length,), dtype=\"int32\")\n",
    "\n",
    "# Embedding and Positional Encoding\n",
    "embedding_layer = layers.Embedding(input_dim=vocab_size, output_dim=d_model, mask_zero=True)\n",
    "x = embedding_layer(inputs)\n",
    "x = PositionalEncoding(max_seq_length, d_model)(x)\n",
    "\n",
    "# Transformer Block\n",
    "def expand_mask(m): \n",
    "    #Import statements needed for inference step\n",
    "    import tensorflow as tf\n",
    "    return tf.expand_dims(m, axis=1)\n",
    "expanded_mask = layers.Lambda(expand_mask, output_shape=(None, 1, max_seq_length))(mask_inputs)\n",
    "attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(\n",
    "    x, x, attention_mask=expanded_mask\n",
    ")\n",
    "attn_output = layers.Dropout(dropout_rate)(attn_output)\n",
    "attn_output = layers.LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "\n",
    "ffn = keras.Sequential([\n",
    "    layers.Dense(ff_dim, activation=\"relu\"),\n",
    "    layers.Dense(d_model),\n",
    "])\n",
    "ffn_output = ffn(attn_output)\n",
    "ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n",
    "sequence_output = layers.LayerNormalization(epsilon=1e-6)(attn_output + ffn_output)\n",
    "\n",
    "# Output layer\n",
    "outputs = layers.Dense(vocab_size, activation=\"softmax\")(sequence_output)\n",
    "\n",
    "# For language modeling, we predict the next token at each position\n",
    "model = keras.Model([inputs, mask_inputs], outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary() #Print out model layers summary\n",
    "\n",
    "#Callback 1: Custom history\n",
    "class BatchLossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "\n",
    "# Instantiate custom callback\n",
    "batch_history = BatchLossHistory()\n",
    "\n",
    "#Callback 2: Checkpointing\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=False, # Set to True to save only the best model\n",
    "    save_freq='epoch'\n",
    ")\n",
    "\n",
    "#Exclude <PAD> from loss function to avoid simply predicting <PAD>\n",
    "pad_token_id = word_to_id[\"<PAD>\"]\n",
    "sample_weights = (y_train != pad_token_id).astype(np.float32)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [X_train, attention_masks],\n",
    "    y_train,\n",
    "    sample_weight=sample_weights,\n",
    "    batch_size=128,\n",
    "    epochs=20,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[batch_history, model_checkpoint_callback]\n",
    ")\n",
    "# Save the model\n",
    "model.save(model_filepath)\n",
    "\n",
    "#Save the training history to subdirectory\n",
    "with open(history_filepath, \"wb\") as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "#Save batch history\n",
    "batch_loss_list = batch_history.losses\n",
    "batch_loss_array = np.array(batch_loss_list)\n",
    "np.save(batch_filepath, batch_loss_array)\n",
    "\n",
    "#Disconnect and delete Colab runtime to save resources\n",
    "runtime.unassign()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd95e8e8",
   "metadata": {},
   "source": [
    "Note: During inference, this from-scratch Tensorflow model performed poorly, with more grammatical errors than LSTM models.  There may be insufficient data for a Transformer model, or some more significant error with the approach used.\n",
    "\n",
    "## Pretrained GPT-2 models with fine-tuning using our datasets "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
