{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52a23bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 23:36:45.975871: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from shared_project_functions import get_target_subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "391adfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_corpus = \"doyleX\"\n",
    "\n",
    "dir = get_target_subdirectory(base_corpus, subdir_string=\"model\")\n",
    "\n",
    "#Load preprocessed data from subdirectory\n",
    "with open(f\"{dir}/{base_corpus}_preprocessed_data.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    X_train = data[\"X_train\"]\n",
    "    y_train = data[\"y_train\"]\n",
    "    word_to_id = data[\"word_to_id\"]\n",
    "    id_to_word = data[\"id_to_word\"],\n",
    "    max_seq_length = data[\"max_seq_length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f3c2312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved padded X_train, y_train, and attention_mask to model_4_doyleX/doyleX_transformer_data.npz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert X_train to numpy array if not already\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Ensure X_train is 2D and padded to max_seq_length\n",
    "if X_train.ndim == 1 or X_train.shape[1] != max_seq_length:\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    X_train = pad_sequences(X_train, maxlen=max_seq_length, padding='post', value=word_to_id[\"<PAD>\"])\n",
    "\n",
    "# Create attention mask: 1 for non-pad, 0 for pad\n",
    "attention_mask = (X_train != word_to_id[\"<PAD>\"]).astype(np.int32)\n",
    "\n",
    "# Save for later use\n",
    "np.savez_compressed(\n",
    "    f\"{dir}/{base_corpus}_transformer_data.npz\",\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    attention_mask=attention_mask,\n",
    "    max_seq_length=max_seq_length\n",
    ")\n",
    "\n",
    "print(f\"Saved padded X_train, y_train, and attention_mask to {dir}/{base_corpus}_transformer_data.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67615550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load data (if not already in memory)\n",
    "data = np.load(f\"{dir}/{base_corpus}_transformer_data.npz\")\n",
    "X_train = data[\"X_train\"]\n",
    "y_train = data[\"y_train\"]\n",
    "attention_mask = data[\"attention_mask\"]\n",
    "max_seq_length = int(data[\"max_seq_length\"])\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "# Positional Encoding Layer\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "        pos = np.arange(max_len)[:, np.newaxis]\n",
    "        i = np.arange(d_model)[np.newaxis, :]\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        angle_rads = pos * angle_rates\n",
    "        pos_encoding = np.zeros((max_len, d_model))\n",
    "        pos_encoding[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        pos_encoding[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        self.pos_encoding = tf.constant(pos_encoding[np.newaxis, ...], dtype=tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        return x + self.pos_encoding[:, :tf.shape(x)[1], :]\n",
    "\n",
    "# Model Hyperparameters\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "ff_dim = 256\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Model Definition\n",
    "inputs = keras.Input(shape=(max_seq_length,), dtype=\"int32\")\n",
    "mask_inputs = keras.Input(shape=(max_seq_length,), dtype=\"int32\")\n",
    "\n",
    "embedding_layer = layers.Embedding(input_dim=vocab_size, output_dim=d_model, mask_zero=True)\n",
    "x = embedding_layer(inputs)\n",
    "x = PositionalEncoding(max_seq_length, d_model)(x)\n",
    "\n",
    "# Transformer Block\n",
    "attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x, attention_mask=tf.expand_dims(mask_inputs, axis=1))\n",
    "attn_output = layers.Dropout(dropout_rate)(attn_output)\n",
    "attn_output = layers.LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "\n",
    "ffn = keras.Sequential([\n",
    "    layers.Dense(ff_dim, activation=\"relu\"),\n",
    "    layers.Dense(d_model),\n",
    "])\n",
    "ffn_output = ffn(attn_output)\n",
    "ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n",
    "sequence_output = layers.LayerNormalization(epsilon=1e-6)(attn_output + ffn_output)\n",
    "\n",
    "# Output layer\n",
    "outputs = layers.Dense(vocab_size, activation=\"softmax\")(sequence_output)\n",
    "\n",
    "# For language modeling, we predict the next token at each position\n",
    "model = keras.Model([inputs, mask_inputs], outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Prepare y_train for categorical prediction at each timestep\n",
    "y_train_seq = np.expand_dims(y_train, -1)  # shape: (num_samples, 1)\n",
    "y_train_seq = np.tile(y_train_seq, (1, max_seq_length))  # shape: (num_samples, max_seq_length)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [X_train, attention_mask],\n",
    "    y_train_seq,\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    validation_split=0.1\n",
    ")\n",
    "# Save the model\n",
    "model.save(f\"doyleX_model.keras\")\n",
    "\n",
    "#Save the training history to subdirectory\n",
    "with open(f\"doyleX_training_history.pkl\", \"wb\") as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mathDL1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
