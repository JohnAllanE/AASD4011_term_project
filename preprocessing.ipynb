{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa4c717c",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "To use this for data preparation, \n",
    "- Prepare a single text file with all training materials.  \n",
    "- Remove as much unneccesary detail as possible (copyrights, tables of contents etc.) \n",
    "- Blank lines will be automatically deleted, but chapter names or other details will remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7b9c81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from shared_project_functions import get_target_subdirectory\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddc0ace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intermediate functions\n",
    "\n",
    "def process_lemmas(corpus_name, sentences, nlp, dir, reset=False):\n",
    "    pkl_file = f\"{corpus_name}_lemmas.pkl\"\n",
    "    \n",
    "    #load from pkl if exists\n",
    "    if not reset and os.path.exists(os.path.join(dir, pkl_file)):\n",
    "        with open(os.path.join(dir, pkl_file), \"rb\") as f:\n",
    "            all_lemmas = pickle.load(f)\n",
    "        print(f\"\\tLoaded lemmas from {os.path.join(dir, pkl_file)}.\")\n",
    "    else:\n",
    "        print(\"\\tProcessing lemmas...\")\n",
    "        processed_docs = nlp.pipe(sentences, batch_size=100, n_process=-1)\n",
    "        all_lemmas = []\n",
    "        for doc in processed_docs:\n",
    "            lemmas = [token.lemma_.lower() for token in doc if token.is_alpha]\n",
    "            all_lemmas.extend(lemmas)\n",
    "        \n",
    "        #save lemmas to pkl file in subdirectory dir\n",
    "        with open(os.path.join(dir, pkl_file), \"wb\") as f:\n",
    "            pickle.dump(all_lemmas, f)\n",
    "        print(f\"\\tSaved lemmas to {os.path.join(dir, pkl_file)}.\")\n",
    "    return all_lemmas\n",
    "\n",
    "def create_mappings(corpus_name, all_lemmas, dir, reset=False):\n",
    "    word_to_id_file = f\"{corpus_name}_word_to_id.json\"\n",
    "    id_to_word_file = f\"{corpus_name}_id_to_word.json\"\n",
    "    \n",
    "    #load from json files if exist\n",
    "    if not reset and os.path.exists(os.path.join(dir, word_to_id_file)) and os.path.exists(os.path.join(dir, id_to_word_file)):\n",
    "        with open(os.path.join(dir, word_to_id_file), \"r\") as f:\n",
    "            word_to_id = json.load(f)\n",
    "        with open(os.path.join(dir, id_to_word_file), \"r\") as f:\n",
    "            id_to_word = json.load(f)\n",
    "        print(f\"\\tLoaded mappings from {word_to_id_file} and {id_to_word_file}.\")\n",
    "    else:\n",
    "        special_tokens = [\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\"]\n",
    "        word_to_id = {token: i for i, token in enumerate(special_tokens)}\n",
    "        next_id = len(special_tokens)\n",
    "        for lemma in set(all_lemmas):\n",
    "            if lemma not in word_to_id:\n",
    "                word_to_id[lemma] = next_id\n",
    "                next_id += 1\n",
    "        id_to_word = {str(id): word for word, id in word_to_id.items()}\n",
    "        with open(os.path.join(dir, word_to_id_file), \"w\") as f:\n",
    "            json.dump(word_to_id, f)\n",
    "        with open(os.path.join(dir, id_to_word_file), \"w\") as f:\n",
    "            json.dump(id_to_word, f)\n",
    "        print(f\"\\tSaved mappings to {os.path.join(dir, word_to_id_file)} and {os.path.join(dir, id_to_word_file)}.\")\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "def create_training_sequences(corpus_name, sentences, nlp, word_to_id, dir, max_seq_length=20, reset=False):\n",
    "    npz_file = f\"{corpus_name}_training_sequences.npz\"\n",
    "    \n",
    "    #load from file if exists\n",
    "    if not reset and os.path.exists(os.path.join(dir, npz_file)):\n",
    "        data = np.load(os.path.join(dir, npz_file))\n",
    "        X_train = data[\"X_train\"]\n",
    "        y_train = data[\"y_train\"]\n",
    "        print(f\"\\tLoaded training data from {os.path.join(dir, npz_file)}.\")\n",
    "    else:\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        for sentence in sentences:\n",
    "            lemmatized_sentence = [token.lemma_.lower() for token in nlp(sentence) if token.is_alpha]\n",
    "            lemmatized_sentence = [\"<SOS>\"] + lemmatized_sentence + [\"<EOS>\"]\n",
    "            numerical_sentence = [word_to_id.get(word, word_to_id[\"<UNK>\"]) for word in lemmatized_sentence]\n",
    "            for i in range(1, len(numerical_sentence)):\n",
    "                input_seq = numerical_sentence[:i]\n",
    "                target_token = numerical_sentence[i]\n",
    "                padded_input_seq = input_seq[:max_seq_length] + [word_to_id[\"<PAD>\"]] * (max_seq_length - len(input_seq))\n",
    "                X_train.append(padded_input_seq)\n",
    "                y_train.append(target_token)\n",
    "        X_train_np = np.array(X_train)\n",
    "        y_train_np = np.array(y_train)\n",
    "        #save to npz file in subdirectory dir\n",
    "        np.savez_compressed(os.path.join(dir, npz_file), X_train=X_train_np, y_train=y_train_np, max_seq_length=max_seq_length)\n",
    "        print(f\"\\tSaved training data to {os.path.join(dir, npz_file)}.\")\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94702b76",
   "metadata": {},
   "outputs": [],
   "source": [
    " #To do:\n",
    "# - Use Tokenizer from Tenensorflow rather than spacy for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2043511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_name: str, method: str = \"default\", max_seq_length: int = 20, reset: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Preprocess the text corpus for training a language model.\n",
    "    \n",
    "\n",
    "    Arguments:\n",
    "        file_name (str): Path to the corpus .txt file.\n",
    "        method (str): Preprocessing method. \"default\" uses spaCy and step-by-step processing.\n",
    "                      (Future methods can be added.)\n",
    "        max_seq_length (int): Maximum sequence length for training samples, needed by model.\n",
    "        reset (bool): If False, uses previously-saved intermediate files to save time.\n",
    "                      If True, starts over and overwrites any files.\n",
    "\n",
    "    Outputs:\n",
    "        Saves a single .pkl file with training data (X, y) and bidirectional mappings (word <-> id).\n",
    "\n",
    "    Examples:\n",
    "        - preprocess(\"shakespeare.txt\")\n",
    "        - preprocess(\"taylor_swift.txt\", reset=True)\n",
    "\n",
    "    To use pkl files in other notebooks:\n",
    "        ```python\n",
    "        with open(\"preprocessed_data.pkl\", \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            X_train = data[\"X_train\"]\n",
    "            y_train = data[\"y_train\"]\n",
    "            word_to_id = data[\"word_to_id\"]\n",
    "            id_to_word = data[\"id_to_word\"]\n",
    "            max_seq_length = data[\"max_seq_length\"]\n",
    "        ```\n",
    "    \"\"\"\n",
    "    import spacy\n",
    "    import pickle\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import json    \n",
    "    \n",
    "    base_argument = file_name.replace(\".txt\", \"\")\n",
    "    print(f\"Preprocessing {base_argument}\")\n",
    "\n",
    "    dir = get_target_subdirectory(corpus_name=base_argument)\n",
    "    \n",
    "    # Load corpus\n",
    "    move_file_flag = False\n",
    "    try: # Load corpus from main directory\n",
    "        with open(os.path.join(file_name), \"r\") as f:\n",
    "            print(f\"\\tOpened {file_name} from main directory.\")\n",
    "            corpus = f.read()\n",
    "            move_file_flag = True\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\tCorpus file {file_name} not found in main directory - trying subdirectory.\")\n",
    "        try: # Load corpus from subdirectory dir\n",
    "            with open(os.path.join(dir, os.path.basename(file_name)), \"r\") as f:\n",
    "                print(f\"\\tOpened {os.path.join(dir, os.path.basename(file_name))} from subdirectory.\")\n",
    "                corpus = f.read()\n",
    "        except FileNotFoundError:\n",
    "            print(f\"\\tCorpus file {os.path.join(dir, os.path.basename(file_name))} not found in subdirectory either.\")\n",
    "            return\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp.max_length = len(corpus) + 10007\n",
    "    sentences = corpus.split('\\n')\n",
    "    sentences = [s for s in sentences if s] #remove empty strings\n",
    "    print(f\"\\t\\tNumber of sentences: {len(sentences)}\")\n",
    "\n",
    "    # Lemmatization\n",
    "    all_lemmas = process_lemmas(base_argument, sentences, nlp, dir, reset)\n",
    "    print(f\"\\t\\tTotal lemmas extracted: {len(all_lemmas)}\")\n",
    "\n",
    "    # Mapping\n",
    "    word_to_id, id_to_word = create_mappings(base_argument, all_lemmas, dir, reset)\n",
    "    print(f\"\\t\\tVocabulary size (including special tokens): {len(word_to_id)}\")\n",
    "\n",
    "    # Training Sequences\n",
    "    X_train, y_train = create_training_sequences(base_argument, sentences, nlp, word_to_id, dir, max_seq_length, reset=reset)\n",
    "    print(f\"\\t\\tNumber of training samples: {len(X_train)}\")\n",
    "\n",
    "    # Save all training data to pkl\n",
    "    data = {\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"word_to_id\": word_to_id,\n",
    "        \"id_to_word\": id_to_word,\n",
    "        \"max_seq_length\": max_seq_length\n",
    "    }\n",
    "    \n",
    "    # Save to pkl file in subdirectory dir\n",
    "    output_filename = f\"{base_argument}_preprocessed_data.pkl\"\n",
    "    with open(os.path.join(dir, output_filename), \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"\\tSaved preprocessed data to {os.path.join(dir, output_filename)}\")\n",
    "    \n",
    "    # Move source file to subdirectory dir\n",
    "    if move_file_flag:\n",
    "        try:\n",
    "            os.rename(file_name, os.path.join(dir, os.path.basename(file_name)))\n",
    "            print(f\"\\tMoved {file_name} to {os.path.join(dir, os.path.basename(file_name))}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\tCould not move {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0671d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing shakespeare\n",
      "\tCorpus file shakespeare.txt not found in main directory - trying subdirectory.\n",
      "\tOpened model_0_shakespeare/shakespeare.txt from subdirectory.\n",
      "\t\tNumber of sentences: 142769\n",
      "\tLoaded lemmas from model_0_shakespeare/shakespeare_lemmas.pkl.\n",
      "\t\tTotal lemmas extracted: 957351\n",
      "\tLoaded mappings from shakespeare_word_to_id.json and shakespeare_id_to_word.json.\n",
      "\t\tVocabulary size (including special tokens): 17927\n",
      "\tLoaded training data from model_0_shakespeare/shakespeare_training_sequences.npz.\n",
      "\t\tNumber of training samples: 1100120\n",
      "\tSaved preprocessed data to model_0_shakespeare/shakespeare_preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "preprocess(\"shakespeare.txt\", reset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6bd1d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c255396d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
