{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd23779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary packages, in case Requirements.txt fails\n",
    "# !pip install spacy transformers numpy pandas datasets scikit-learn matplotlib evaluate wandb torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0f10b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/lfdv6s24?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f31e3ac8ad0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import spacy\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "import wandb\n",
    "wandb.init(mode=\"disabled\") \n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4456c95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning and Normalization Pipeline\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_shakespeare(text: str) -> str:\n",
    "\n",
    "    # remove any ALL CAPS words (Titles, character names, stage directions, etc.)\n",
    "    text = re.sub(r'\\b[A-Z]{2,}\\.', '', text)\n",
    "    text = re.sub(r'\\b[A-Z]{2,}\\b', '', text)\n",
    "\n",
    "    # lowercase normalization\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove ACT/SCENE markers\n",
    "    text = re.sub(r'\\bact\\s+[ivx]+\\b', '', text)\n",
    "    text = re.sub(r'\\bscene\\s+[ivx]+\\b', '', text)\n",
    "\n",
    "    # remove bracketed text [ ... ]\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "\n",
    "    # remove standalone numbers (often sonnet numbers or line counts)\n",
    "    text = re.sub(r'^\\s*\\d+\\s*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # remove play headers and all-uppercase lines (likely metadata, not verse)\n",
    "    text = re.sub(r'^[A-Z\\s]{3,}$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # collapse multiple newlines\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "\n",
    "    # remove '\\n' as our model will not need to generate new lines in the style of Sonnets\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    # strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Clean the file\n",
    "TXT_PATH = \"./model_0_shakespeare/shakespeare.txt\"\n",
    "CLEANED_TXT_PATH = \"./model_0_shakespeare/shakespeare_cleaned.txt\"\n",
    "\n",
    "with open(TXT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "cleaned_text = clean_shakespeare(raw_text)\n",
    "\n",
    "with open(CLEANED_TXT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0f74656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 70932\n"
     ]
    }
   ],
   "source": [
    "# Load Text\n",
    "with open(CLEANED_TXT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = f.read()\n",
    "\n",
    "# Sentence segmentation (extract sentences from .txt)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = len(corpus) + 1000  # handle long corpus\n",
    "\n",
    "doc = nlp(corpus)\n",
    "sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "\n",
    "print(f\"Number of sentences: {len(sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b8ace95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Example Sentences ===\n",
      "1: the complete works of william shakespeare by william shakespeare contents   ‚Äôs        ,    ‚Äôs ‚Äôs  a  ‚Äôs     ,     ,     ; ,     ‚Äôs  a ‚Äôs  from fairest creatures we desire increase, that thereby beauty‚Äôs rose might never die, but as the riper should by time decease, his tender heir might bear his memory: but thou contracted to thine own bright eyes, feed‚Äôst thy light‚Äôs flame with self-substantial fuel, making a famine where abundance lies, thyself thy foe, to thy sweet self too cruel: thou that art now the world‚Äôs fresh ornament, and only herald to the gaudy spring, within thine own bud buriest thy content, and, tender churl, mak‚Äôst waste in niggarding:     pity the world, or else this glutton be,     to eat the world‚Äôs due, by the grave and thee.\n",
      "2: when forty winters shall besiege thy brow, and dig deep trenches in thy beauty‚Äôs field, thy youth‚Äôs proud livery so gazed on now, will be a tattered weed of small worth held: then being asked, where all thy beauty lies, where all the treasure of thy lusty days; to say, within thine own deep sunken eyes, were an all-eating shame, and thriftless praise.\n",
      "3: how much more praise deserv‚Äôd thy beauty‚Äôs use, if thou couldst answer ‚Äòthis fair child of mine shall sum my count, and make my old excuse,‚Äô proving his beauty by succession thine.\n",
      "4: this were to be new made when thou art old,     and see thy blood warm when thou feel‚Äôst it cold.\n",
      "5: look in thy glass and tell the face thou viewest, now is the time that face should form another, whose fresh repair if now thou not renewest, thou dost beguile the world, unbless some mother.\n",
      "6: for where is she so fair whose uneared womb disdains the tillage of thy husbandry? or who is he so fond will be the tomb of his self-love to stop posterity?\n",
      "7: thou art thy mother‚Äôs glass\n",
      "8: and she in thee calls back the lovely april of her prime, so thou through windows of thine age shalt see, despite of wrinkles this thy golden time.\n",
      "9: but if thou live remembered not to be,     die single and thine image dies with thee.\n",
      "10: unthrifty loveliness why dost thou spend, upon thyself thy beauty‚Äôs legacy?\n",
      "\n",
      "Total sentences: 70932\n"
     ]
    }
   ],
   "source": [
    "# Explore sentences\n",
    "\n",
    "print(\"=== Example Sentences ===\")\n",
    "for i, s in enumerate(sentences[:10]):  # show first 10\n",
    "    print(f\"{i+1}: {s}\")\n",
    "print(\"\\nTotal sentences:\", len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "928425e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Example Dataset Entries ===\n",
      "1: {'text': 'the complete works of william shakespeare by william shakespeare contents   ‚Äôs        ,    ‚Äôs ‚Äôs  a  ‚Äôs     ,     ,     ; ,     ‚Äôs  a ‚Äôs  from fairest creatures we desire increase, that thereby beauty‚Äôs rose might never die, but as the riper should by time decease, his tender heir might bear his memory: but thou contracted to thine own bright eyes, feed‚Äôst thy light‚Äôs flame with self-substantial fuel, making a famine where abundance lies, thyself thy foe, to thy sweet self too cruel: thou that art now the world‚Äôs fresh ornament, and only herald to the gaudy spring, within thine own bud buriest thy content, and, tender churl, mak‚Äôst waste in niggarding:     pity the world, or else this glutton be,     to eat the world‚Äôs due, by the grave and thee.'}\n",
      "2: {'text': 'when forty winters shall besiege thy brow, and dig deep trenches in thy beauty‚Äôs field, thy youth‚Äôs proud livery so gazed on now, will be a tattered weed of small worth held: then being asked, where all thy beauty lies, where all the treasure of thy lusty days; to say, within thine own deep sunken eyes, were an all-eating shame, and thriftless praise.'}\n",
      "3: {'text': 'how much more praise deserv‚Äôd thy beauty‚Äôs use, if thou couldst answer ‚Äòthis fair child of mine shall sum my count, and make my old excuse,‚Äô proving his beauty by succession thine.'}\n",
      "4: {'text': 'this were to be new made when thou art old,     and see thy blood warm when thou feel‚Äôst it cold.'}\n",
      "5: {'text': 'look in thy glass and tell the face thou viewest, now is the time that face should form another, whose fresh repair if now thou not renewest, thou dost beguile the world, unbless some mother.'}\n",
      "6: {'text': 'for where is she so fair whose uneared womb disdains the tillage of thy husbandry? or who is he so fond will be the tomb of his self-love to stop posterity?'}\n",
      "7: {'text': 'thou art thy mother‚Äôs glass'}\n",
      "8: {'text': 'and she in thee calls back the lovely april of her prime, so thou through windows of thine age shalt see, despite of wrinkles this thy golden time.'}\n",
      "9: {'text': 'but if thou live remembered not to be,     die single and thine image dies with thee.'}\n",
      "10: {'text': 'unthrifty loveliness why dost thou spend, upon thyself thy beauty‚Äôs legacy?'}\n",
      "\n",
      "Dataset length: 70932\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face Dataset\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": sentences})\n",
    "\n",
    "# Explore Dataset\n",
    "\n",
    "print(\"\\n=== Example Dataset Entries ===\")\n",
    "for i in range(10):  # show first 10\n",
    "    print(f\"{i+1}: {dataset[i]}\")\n",
    "print(\"\\nDataset length:\", len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3bd2cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d51867848554a7d83d027ec39a7576a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/70932 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load pre-trained model\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# GPT-2 doesn‚Äôt have a padding token by default\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokens = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "        #return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "241d150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to use on WSL for training with GPU\n",
    "\n",
    "#tokenized_dataset.save_to_disk(\"shakespeare_tokenized\")\n",
    "\n",
    "\n",
    "#Open on WSL \n",
    "\n",
    "#tokenized_dataset = load_from_disk(\"shakespeare_tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "effae1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train/eval\n",
    "split = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split[\"train\"]\n",
    "eval_dataset = split[\"test\"]\n",
    "\n",
    "# Metric: Perplexity (based on loss)\n",
    "# Hugging Face's `evaluate` doesn't include perplexity directly,\n",
    "# but we can derive it from cross-entropy loss.\n",
    "def compute_metrics(eval_pred):\n",
    "    loss = eval_pred.metrics[\"eval_loss\"] if \"eval_loss\" in eval_pred.metrics else None\n",
    "    if loss is None:\n",
    "        return {}\n",
    "    perplexity = np.exp(loss)\n",
    "    return {\"perplexity\": perplexity, \"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a54f3e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations for Training NO IDEA IF IT WORKS\n",
    "\n",
    "import transformers\n",
    "from transformers import TrainerCallback\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "eval_steps = []\n",
    "\n",
    "def plot_metrics():\n",
    "    clear_output(wait=True)\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(eval_steps, eval_losses, label=\"Eval Loss\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "class PlotCallback(transformers.TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            if \"loss\" in logs:\n",
    "                train_losses.append(logs[\"loss\"])\n",
    "            if \"eval_loss\" in logs:\n",
    "                eval_losses.append(logs[\"eval_loss\"])\n",
    "                eval_steps.append(state.global_step)\n",
    "                plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95576100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: /home/alvisngan/Documents/george_brown/math_dl/AASD4011_term_project/.venv/bin/python\n",
      "torch: 2.7.1+cu126 /home/alvisngan/Documents/george_brown/math_dl/AASD4011_term_project/.venv/lib/python3.13/site-packages/torch/__init__.py\n",
      "transformers: 4.56.2 /home/alvisngan/Documents/george_brown/math_dl/AASD4011_term_project/.venv/lib/python3.13/site-packages/transformers/__init__.py\n",
      "accelerate: 1.1.0 /home/alvisngan/Documents/george_brown/math_dl/AASD4011_term_project/.venv/lib/python3.13/site-packages/accelerate/__init__.py\n",
      "tensorboard: 2.20.0 /home/alvisngan/Documents/george_brown/math_dl/AASD4011_term_project/.venv/lib/python3.13/site-packages/tensorboard/__init__.py\n",
      "Trainer OK.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\nWorks for me - Alvis\\npython          3.13.7\\ntorch           2.7.1+cu126\\ntransformers    4.56.2\\naccelerate      1.1.0\\ntensorboard     2.20.0\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show versions + import paths (DEBUG)\n",
    "import sys\n",
    "import torch, transformers, accelerate, tensorboard\n",
    "print(\"python:\", sys.executable)\n",
    "print(\"torch:\", torch.__version__, getattr(torch, \"__file__\", None))\n",
    "print(\"transformers:\", transformers.__version__, getattr(transformers, \"__file__\", None))\n",
    "print(\"accelerate:\", accelerate.__version__, getattr(accelerate, \"__file__\", None))\n",
    "print(\"tensorboard:\", tensorboard.__version__, getattr(tensorboard, \"__file__\", None))\n",
    "\n",
    "# 3) Minimal Trainer dry run to ensure import path is sane\n",
    "from transformers import TrainingArguments, Trainer\n",
    "print(\"Trainer OK.\")\n",
    "\n",
    "\"\"\" \n",
    "Works for me - Alvis\n",
    "python          3.13.7\n",
    "torch           2.7.1+cu126\n",
    "transformers    4.56.2\n",
    "accelerate      1.1.0\n",
    "tensorboard     2.20.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b1607d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate, inspect\n",
    "\n",
    "# FIX TRANSFORMER (IDK LOL IT'S STUPID)\n",
    "\n",
    "# Only patch if needed, and only once\n",
    "_unwrap = accelerate.Accelerator.unwrap_model\n",
    "sig = str(inspect.signature(_unwrap))\n",
    "\n",
    "if \"keep_torch_compile\" not in sig and not getattr(_unwrap, \"_patched_keep_torch_compile\", False):\n",
    "    _orig_unwrap = _unwrap\n",
    "    def _unwrap_compat(self, model, *args, **kwargs):\n",
    "        # drop unknown kwarg if present\n",
    "        kwargs.pop(\"keep_torch_compile\", None)\n",
    "        return _orig_unwrap(self, model, *args, **kwargs)\n",
    "    _unwrap_compat._patched_keep_torch_compile = True\n",
    "    accelerate.Accelerator.unwrap_model = _unwrap_compat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6dc9e110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='201' max='23940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  201/23940 02:40 < 5:18:23, 1.24 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='330' max='7094' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 330/7094 04:13 < 1:26:54, 1.30 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 87\u001b[39m\n\u001b[32m     76\u001b[39m trainer = Trainer(\n\u001b[32m     77\u001b[39m     model=model,\n\u001b[32m     78\u001b[39m     args=args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     82\u001b[39m     callbacks=[PlotCallback()],   \u001b[38;5;66;03m# üëà add live plotting\u001b[39;00m\n\u001b[32m     83\u001b[39m )\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m#Train the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/george_brown/math_dl/AASD4011_term_project/.venv/lib/python3.13/site-packages/transformers/trainer.py:2328\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/george_brown/math_dl/AASD4011_term_project/.venv/lib/python3.13/site-packages/transformers/trainer.py:2754\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2752\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.epoch = epoch + (step + \u001b[32m1\u001b[39m + steps_skipped) / steps_in_epoch\n\u001b[32m   2753\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2754\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2755\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2756\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2757\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2758\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2759\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2760\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2761\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2762\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2763\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2764\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2765\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_substep_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/george_brown/math_dl/AASD4011_term_project/.venv/lib/python3.13/site-packages/transformers/trainer.py:3227\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3225\u001b[39m metrics = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3226\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_evaluate:\n\u001b[32m-> \u001b[39m\u001b[32m3227\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3228\u001b[39m     is_new_best_metric = \u001b[38;5;28mself\u001b[39m._determine_best_metric(metrics=metrics, trial=trial)\n\u001b[32m   3230\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy == SaveStrategy.BEST:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/george_brown/math_dl/AASD4011_term_project/.venv/lib/python3.13/site-packages/transformers/trainer.py:3176\u001b[39m, in \u001b[36mTrainer._evaluate\u001b[39m\u001b[34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[39m\n\u001b[32m   3175\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m3176\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3177\u001b[39m     \u001b[38;5;28mself\u001b[39m._report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m.state.global_step, metrics)\n\u001b[32m   3179\u001b[39m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/george_brown/math_dl/AASD4011_term_project/.venv/lib/python3.13/site-packages/transformers/trainer.py:4469\u001b[39m, in \u001b[36mTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4466\u001b[39m start_time = time.time()\n\u001b[32m   4468\u001b[39m eval_loop = \u001b[38;5;28mself\u001b[39m.prediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.use_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evaluation_loop\n\u001b[32m-> \u001b[39m\u001b[32m4469\u001b[39m output = \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4470\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4471\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvaluation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4472\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[32m   4473\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[32m   4474\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4475\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4476\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4477\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4479\u001b[39m total_batch_size = \u001b[38;5;28mself\u001b[39m.args.eval_batch_size * \u001b[38;5;28mself\u001b[39m.args.world_size\n\u001b[32m   4480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_jit_compilation_time\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output.metrics:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/george_brown/math_dl/AASD4011_term_project/.venv/lib/python3.13/site-packages/transformers/trainer.py:4717\u001b[39m, in \u001b[36mTrainer.evaluation_loop\u001b[39m\u001b[34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4715\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m args.eval_accumulation_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (step + \u001b[32m1\u001b[39m) % args.eval_accumulation_steps == \u001b[32m0\u001b[39m:\n\u001b[32m   4716\u001b[39m     all_losses.to_cpu_and_numpy()\n\u001b[32m-> \u001b[39m\u001b[32m4717\u001b[39m     \u001b[43mall_preds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_cpu_and_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4718\u001b[39m     all_labels.to_cpu_and_numpy()\n\u001b[32m   4719\u001b[39m     all_inputs.to_cpu_and_numpy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/george_brown/math_dl/AASD4011_term_project/.venv/lib/python3.13/site-packages/transformers/trainer_pt_utils.py:330\u001b[39m, in \u001b[36mEvalLoopContainer.to_cpu_and_numpy\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    328\u001b[39m     \u001b[38;5;28mself\u001b[39m.arrays = new_arrays\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.do_nested_concat:\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m     \u001b[38;5;28mself\u001b[39m.arrays = \u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_arrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    332\u001b[39m     \u001b[38;5;28mself\u001b[39m.arrays.extend(new_arrays)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/george_brown/math_dl/AASD4011_term_project/.venv/lib/python3.13/site-packages/transformers/trainer_pt_utils.py:137\u001b[39m, in \u001b[36mnested_concat\u001b[39m\u001b[34m(tensors, new_tensors, padding_index)\u001b[39m\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\n\u001b[32m    134\u001b[39m         {k: nested_concat(t, new_tensors[k], padding_index=padding_index) \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensors.items()}\n\u001b[32m    135\u001b[39m     )\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, np.ndarray):\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnumpy_pad_and_concatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported type for concatenation: got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/george_brown/math_dl/AASD4011_term_project/.venv/lib/python3.13/site-packages/transformers/trainer_pt_utils.py:107\u001b[39m, in \u001b[36mnumpy_pad_and_concatenate\u001b[39m\u001b[34m(array1, array2, padding_index)\u001b[39m\n\u001b[32m    104\u001b[39m array2 = atleast_1d(array2)\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(array1.shape) == \u001b[32m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m array1.shape[\u001b[32m1\u001b[39m] == array2.shape[\u001b[32m1\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marray2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# Let's figure out the new shape\u001b[39;00m\n\u001b[32m    110\u001b[39m new_shape = (array1.shape[\u001b[32m0\u001b[39m] + array2.shape[\u001b[32m0\u001b[39m], \u001b[38;5;28mmax\u001b[39m(array1.shape[\u001b[32m1\u001b[39m], array2.shape[\u001b[32m1\u001b[39m])) + array1.shape[\u001b[32m2\u001b[39m:]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"C:/Users/Gaels/MATH-Proj-GPT2/shakespeare_gpt2\",\n",
    "#     #eval_strategy=\"epoch\",\n",
    "#     learning_rate=5e-5,\n",
    "#     per_device_train_batch_size=8,\n",
    "#     per_device_eval_batch_size=8,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "#     save_strategy=\"epoch\",\n",
    "#     disable_tqdm=False,\n",
    "#     logging_dir=\"C:/Users/Gaels/MATH-Proj-GPT2/shakespeare_logs\",\n",
    "#     logging_steps=50,\n",
    "#     logging_strategy=\"steps\",\n",
    "#     eval_steps=200,\n",
    "#     logging_first_step=True,\n",
    "# )\n",
    "\n",
    "# args = TrainingArguments(\n",
    "#     output_dir=\"./shakespeare_gpt2\",\n",
    "#     logging_dir=\"./shakespeare_logs\",\n",
    "#     per_device_train_batch_size=1,\n",
    "#     per_device_eval_batch_size=1,\n",
    "#     gradient_accumulation_steps=8,\n",
    "#     learning_rate=5e-5,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "#     save_strategy=\"epoch\",\n",
    "#     eval_strategy=\"steps\",      # ‚Üê use this\n",
    "#     eval_steps=200,\n",
    "#     logging_strategy=\"steps\",\n",
    "#     logging_steps=50,\n",
    "#     report_to=[\"tensorboard\"],\n",
    "#     fp16=True,                  # OK on your GPU\n",
    "# )\n",
    "\n",
    "# --- Memory helpers (do this before Trainer) ---\n",
    "model.config.use_cache = False\n",
    "if hasattr(model, \"gradient_checkpointing_enable\"):\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "# --- Build args in a version-safe way ---\n",
    "base_args = dict(\n",
    "    output_dir=\"./shakespeare_gpt2\",\n",
    "    logging_dir=\"./shakespeare_logs\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,                      # works on 1070 Ti\n",
    "    report_to=[\"tensorboard\"],\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",          # keep epoch saves (lighter on VRAM spikes)\n",
    "    eval_steps=200,\n",
    "    eval_accumulation_steps=1,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "params = inspect.signature(TrainingArguments).parameters\n",
    "if \"evaluation_strategy\" in params:          # HF v4.x\n",
    "    base_args[\"evaluation_strategy\"] = \"steps\"\n",
    "elif \"eval_strategy\" in params:              # HF v5.x\n",
    "    base_args[\"eval_strategy\"] = \"steps\"\n",
    "# else: leave eval off if neither exists\n",
    "\n",
    "args = TrainingArguments(**base_args)\n",
    "\n",
    "\n",
    "#Initialize Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[PlotCallback()],   # üëà add live plotting\n",
    ")\n",
    "\n",
    "#Train the model\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4a9342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model + Tokenizer\n",
    "\n",
    "save_dir = \"shakespeare_gpt2_final\"\n",
    "trainer.save_model(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28521f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation\n",
    "\n",
    "# 1. Perplexity Evaluation\n",
    "print(\"\\n--- Perplexity Evaluation ---\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation Loss: {eval_results['eval_loss']}\")\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss'])}\")\n",
    "\n",
    "# 3. BLEU/ROUGE Scores\n",
    "print(\"\\n--- BLEU/ROUGE Evaluation ---\")\n",
    "import evaluate\n",
    "metric = evaluate.load(\"rouge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3518318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference (text generation)\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=save_dir,\n",
    "    tokenizer=save_dir,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(\"\\n=== Sample Shakespearean Text ===\\n\")\n",
    "print(generator(\"to be, or not to be\", max_length=100, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
