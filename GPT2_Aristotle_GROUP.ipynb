{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd23779e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f10b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     -------- ------------------------------- 2.6/12.8 MB 15.1 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 13.4 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 15.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 15.4 MB/s  0:00:00\n",
      "âœ” Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nimport torch\\nimport torchvision\\nimport torchvision.transforms as transforms\\n\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nimport torch.optim as optim\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import spacy\n",
    "from datasets import Dataset \n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "import wandb\n",
    "wandb.init(mode=\"disabled\") \n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4456c95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning and Normalization Pipeline\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_aristotle(text: str) -> str:\n",
    "\n",
    "    # remove any ALL CAPS words (Titles, character names, stage directions, etc.)\n",
    "    text = re.sub(r'\\b[A-Z]{2,}\\.', '', text)\n",
    "    text = re.sub(r'\\b[A-Z]{2,}\\b', '', text)\n",
    "\n",
    "    # Remove \"Part X,\" or \"BOOK X,\" (case-insensitive)\n",
    "    text = re.sub(r'\\b(?:Part|BOOK)\\s+\\w+,?', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove things like (12), (3a), etc.\n",
    "    text = re.sub(r'\\(\\s*[\\w\\d]+\\s*\\)', '', text)\n",
    "    \n",
    "    # Remove solitary quotes (\")\n",
    "    text = text.replace('\"', '')\n",
    "    \n",
    "    # Remove long lines of dashes (3 or more in a row)\n",
    "    text = re.sub(r'-{3,}', '', text)\n",
    "    \n",
    "    # Collapse multiple spaces into one\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # lowercase normalization\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove standalone numbers (often sonnet numbers or line counts)\n",
    "    text = re.sub(r'^\\s*\\d+\\s*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # remove play headers and all-uppercase lines (likely metadata, not verse)\n",
    "    text = re.sub(r'^[A-Z\\s]{3,}$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # collapse multiple newlines\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "\n",
    "    # remove '\\n' as our model will not need to generate new lines\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    # strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Clean the file\n",
    "with open(\"aristotle.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "cleaned_text = clean_aristotle(raw_text)\n",
    "\n",
    "with open(\"aristotle_cleaned.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0f74656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 4267\n"
     ]
    }
   ],
   "source": [
    "# Load Text\n",
    "with open(\"aristotle_cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = f.read()\n",
    "\n",
    "# Sentence segmentation (extract sentences from .txt)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = len(corpus) + 1000  # handle long corpus\n",
    "\n",
    "doc = nlp(corpus)\n",
    "sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "\n",
    "print(f\"Number of sentences: {len(sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b8ace95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Example Sentences ===\n",
      "1: things are said to be named 'equivocally' when, though they have a common name, the definition corresponding with the name differs for each.\n",
      "2: thus, a real man and a figure in a picture can both lay claim to the name 'animal'; yet these are equivocally so named, for, though they have a common name, the definition corresponding with the name differs for each.\n",
      "3: for should any one define in what sense each is an animal, his definition in the one case will be appropriate to that case only.\n",
      "4: on the other hand, things are said to be named 'univocally' which have both the name and the definition answering to the name in common.\n",
      "5: a man and an ox are both 'animal', and these are univocally so named, inasmuch as not only the name, but also the definition, is the same in both cases: for if a man should state in what sense each is an animal, the statement in the one case would be identical with that in the other.\n",
      "6: things are said to be named 'derivatively', which derive their name from some other name, but differ from it in termination.\n",
      "7: thus the grammarian derives his name from the word 'grammar', and the courageous man from the word 'courage'.\n",
      "8: forms of speech are either simple or composite.\n",
      "9: examples of the latter are such expressions as 'the man runs', 'the man wins'; of the former 'man', 'ox', 'runs', 'wins'.\n",
      "10: of things themselves some are predicable of a subject, and are never present in a subject.\n",
      "\n",
      "Total sentences: 4267\n"
     ]
    }
   ],
   "source": [
    "# Explore sentences\n",
    "\n",
    "print(\"=== Example Sentences ===\")\n",
    "for i, s in enumerate(sentences[:10]):  # show first 10\n",
    "    print(f\"{i+1}: {s}\")\n",
    "print(\"\\nTotal sentences:\", len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "928425e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Example Dataset Entries ===\n",
      "1: {'text': \"things are said to be named 'equivocally' when, though they have a common name, the definition corresponding with the name differs for each.\"}\n",
      "2: {'text': \"thus, a real man and a figure in a picture can both lay claim to the name 'animal'; yet these are equivocally so named, for, though they have a common name, the definition corresponding with the name differs for each.\"}\n",
      "3: {'text': 'for should any one define in what sense each is an animal, his definition in the one case will be appropriate to that case only.'}\n",
      "4: {'text': \"on the other hand, things are said to be named 'univocally' which have both the name and the definition answering to the name in common.\"}\n",
      "5: {'text': \"a man and an ox are both 'animal', and these are univocally so named, inasmuch as not only the name, but also the definition, is the same in both cases: for if a man should state in what sense each is an animal, the statement in the one case would be identical with that in the other.\"}\n",
      "6: {'text': \"things are said to be named 'derivatively', which derive their name from some other name, but differ from it in termination.\"}\n",
      "7: {'text': \"thus the grammarian derives his name from the word 'grammar', and the courageous man from the word 'courage'.\"}\n",
      "8: {'text': 'forms of speech are either simple or composite.'}\n",
      "9: {'text': \"examples of the latter are such expressions as 'the man runs', 'the man wins'; of the former 'man', 'ox', 'runs', 'wins'.\"}\n",
      "10: {'text': 'of things themselves some are predicable of a subject, and are never present in a subject.'}\n",
      "\n",
      "Dataset length: 4267\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face Dataset\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": sentences})\n",
    "\n",
    "# Explore Dataset\n",
    "\n",
    "print(\"\\n=== Example Dataset Entries ===\")\n",
    "for i in range(10):  # show first 10\n",
    "    print(f\"{i+1}: {dataset[i]}\")\n",
    "print(\"\\nDataset length:\", len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3bd2cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec507b4526fc4a25b941dfdcb725d3d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4267 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b1a6ea43b74eb7984cb1ddcf52d562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4267 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load pre-trained model\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# GPT-2 doesnâ€™t have a padding token by default\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokens = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=64,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].clone()\n",
    "    return tokens\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_dataset.save_to_disk(\"aristotle_tokenized_to_colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54f3e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations for Training\n",
    "\n",
    "'''\n",
    "import transformers\n",
    "from transformers import TrainerCallback\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "eval_steps = []\n",
    "\n",
    "def plot_metrics():\n",
    "    clear_output(wait=True)\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(eval_steps, eval_losses, label=\"Eval Loss\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "class PlotCallback(transformers.TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            if \"loss\" in logs:\n",
    "                train_losses.append(logs[\"loss\"])\n",
    "            if \"eval_loss\" in logs:\n",
    "                eval_losses.append(logs[\"eval_loss\"])\n",
    "                eval_steps.append(state.global_step)\n",
    "                plot_metrics()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc9e110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SWITCH TO COLAB, WITH TOKENIZED_DATASET UPLOADED\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load pre-trained model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Split dataset into train/eval\n",
    "tokenized_dataset = load_from_disk(\"aristotle_tokenized_to_colab\")\n",
    "split = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split[\"train\"]\n",
    "eval_dataset = split[\"test\"]\n",
    "\n",
    "# Training arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ARISTOTLE\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"no\",\n",
    "    eval_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    eval_steps=50,    \n",
    "    disable_tqdm=False,\n",
    "    logging_dir=\"////your_path///////ARISTOTLE\",\n",
    "    logging_first_step=True,\n",
    "    report_to=[\"none\"]\n",
    ")\n",
    "\n",
    "#Initialize Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    #compute_metrics=compute_metrics,\n",
    "    #callbacks=[PlotCallback],   # ðŸ‘ˆ add live plotting\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2842b453",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4a9342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:/Users/Gaels/MATH-Proj-GPT2/shakespeare_gpt2_final\\\\tokenizer_config.json',\n",
       " 'C:/Users/Gaels/MATH-Proj-GPT2/shakespeare_gpt2_final\\\\special_tokens_map.json',\n",
       " 'C:/Users/Gaels/MATH-Proj-GPT2/shakespeare_gpt2_final\\\\vocab.json',\n",
       " 'C:/Users/Gaels/MATH-Proj-GPT2/shakespeare_gpt2_final\\\\merges.txt',\n",
       " 'C:/Users/Gaels/MATH-Proj-GPT2/shakespeare_gpt2_final\\\\added_tokens.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save Model + Tokenizer\n",
    "\n",
    "save_dir = \"aristotle_gpt2_trained\"\n",
    "trainer.save_model(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"./trained_model/training_metrics.json\", \"w\") as f:\n",
    "    json.dump(trainer.state.log_history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0b79e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACK TO LOCAL MACHINE AFTER COLAB TRAINING, WITH aristotle_gpt2_trained FOLDER + training_metrics.json DOWNLOADED\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, Trainer, pipeline\n",
    "\n",
    "local_path = r\"////////your_entire_path/////////aristotle_trained_gpt2_model\"\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(local_path)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(local_path)\n",
    "\n",
    "# Load metrics\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"////////your_entire_path/////////training_metrics.json\") as f:\n",
    "    history = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217d3536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity Evaluation\n",
    "\n",
    "'''\n",
    "# Metric: Perplexity (based on loss)\n",
    "# Hugging Face's `evaluate` doesn't include perplexity directly,\n",
    "# but we can derive it from cross-entropy loss.\n",
    "def compute_metrics(eval_pred):\n",
    "    loss = eval_pred.metrics[\"eval_loss\"] if \"eval_loss\" in eval_pred.metrics else None\n",
    "    if loss is None:\n",
    "        return {}\n",
    "    perplexity = np.exp(loss)\n",
    "    return {\"perplexity\": perplexity, \"loss\": loss}\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "metrics = compute_metrics(eval_results)\n",
    "\n",
    "print(\"Evaluation results with GPT-2:\")\n",
    "print(metrics)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28521f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation\n",
    "\n",
    "'''\n",
    "# Load model from folder\n",
    "model_path = local_path\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "\n",
    "# BLEU/ROUGE Scores\n",
    "print(\"\\n--- BLEU/ROUGE Evaluation ---\")\n",
    "import evaluate\n",
    "metric = evaluate.load(\"rouge\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3518318d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=70) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample Aristotelian Text ===\n",
      "\n",
      "\n",
      "--- Output 1 ---\n",
      "what if a man-for-hisself, who is not hisself, is not in that state of being but is in the same state of being, and also is not in the same state of being but is in the same state of being?\n",
      "\n",
      "--- Output 2 ---\n",
      "what if a man is not at home yet?\n",
      "\n",
      "--- Output 3 ---\n",
      "what if a man, then, is said to be one, as when the latter say he is not one, but he is one?\n",
      "\n",
      "--- Output 4 ---\n",
      "what if a man is not a man, but a horse?\n",
      "\n",
      "--- Output 5 ---\n",
      "what if a man also said that the earth is a substance?\n",
      "\n",
      "--- Output 6 ---\n",
      "what if a man were to say, 'the world is a cube'; and, again, if the people who composed these statements were to say so, the one and the same thing would have to be different; for the one would be better and the other worse, but the one and the same thing would be neither worse nor worse than the other.\n",
      "\n",
      "--- Output 7 ---\n",
      "what if a man is white or is not white?\n",
      "\n",
      "--- Output 8 ---\n",
      "what if a man is present, what must happen to him, and what will be his cause of it?\n",
      "\n",
      "--- Output 9 ---\n",
      "what if a man is not a man, but a statue?\n",
      "\n",
      "--- Output 10 ---\n",
      "what if a man sees, and perceives the same thing, in another manner, and sees not only the same thing in that which he sees, but also the same thing in another manner, though not the same thing itself?\n"
     ]
    }
   ],
   "source": [
    "# Inference (text generation)\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "prompt = \"what if a man\"\n",
    "\n",
    "print(\"\\n=== Sample Aristotelian Text ===\\n\")\n",
    "\n",
    "outputs = generator(\n",
    "    prompt,\n",
    "    max_length=70,\n",
    "    num_return_sequences=10,   # ðŸ‘ˆ generate 10 completions\n",
    "    do_sample=True,            # ðŸ‘ˆ enable sampling\n",
    "    top_k=100,                  # limit to top-k tokens\n",
    "    top_p=0.95,                 # nucleus sampling\n",
    "    temperature=0.8             # control creativity\n",
    ")\n",
    "\n",
    "for i, out in enumerate(outputs, 1):\n",
    "    print(f\"\\n--- Output {i} ---\\n{out['generated_text']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
